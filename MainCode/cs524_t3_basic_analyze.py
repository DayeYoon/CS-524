# -*- coding: utf-8 -*-
"""CS524- T3-Basic_Analyze

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-UHPiovzpuLfi5pQxZQZt7XQESub-4OK

Dataset link:


*   [Traffic Crashes](https://data.cityofchicago.org/Transportation/Traffic-Crashes-Crashes/85ca-t3if/about_data)
*   [Chicago Street](https://data.cityofchicago.org/Transportation/Street-Center-Lines/6imu-meau)
*   [Chicago Boundary](https://data.cityofchicago.org/Facilities-Geographic-Boundaries/Boundaries-City/ewy2-6yfk)

## 0. Run the cells below before running any session
"""

!pip install pandas pysolar pytz
!pip install sodapy
!pip install geojson
!pip install pydeck

import pandas as pd
import requests
import altair as alt
import matplotlib.pyplot as plt
from sodapy import Socrata
from shapely.geometry import Polygon, MultiPolygon
from shapely.geometry import shape
import geojson
import numpy as np

from google.colab import drive
drive.mount('/content/gdrive')

path = "/content/gdrive/MyDrive/CS 524/CS 524 - shared folder/Project/Dataset"

import pydeck as pdk
import geopandas as gpd
from shapely.geometry import Point, LineString, Polygon

# Load the datasets
Building_Height_File = path + '/ChicagoBuildingsHeight_OpenStreetMap.geojson'
Chicago_Boundary_File = path + '/Chicago_City_Boundary.csv'
Chicago_Streets_File = path + '/Chicago_Streets.csv'

gdf_buildings = gpd.read_file(Building_Height_File)

# Load and process the boundary CSV
df_boundary = pd.read_csv(Chicago_Boundary_File)

# Load and process the streets CSV
df_streets = pd.read_csv(Chicago_Streets_File)

# Convert the 'the_geom' column to a geometry object
gdf_boundary = gpd.GeoDataFrame(df_boundary, geometry=gpd.GeoSeries.from_wkt(df_boundary['the_geom']))
# Convert the 'the_geom' column to a geometry object
gdf_streets = gpd.GeoDataFrame(df_streets, geometry=gpd.GeoSeries.from_wkt(df_streets['the_geom']))

# Set an initial CRS for boundary and streets if not already set
gdf_boundary = gdf_boundary.set_crs('EPSG:4326')
gdf_streets = gdf_streets.set_crs('EPSG:4326')

gdf_buildings = gdf_buildings.to_crs('EPSG:4326')
gdf_boundary = gdf_boundary.to_crs('EPSG:4326')
gdf_streets = gdf_streets.to_crs('EPSG:4326')

# Convert height to numeric, replacing non-numeric values with a default height
gdf_buildings['height'] = pd.to_numeric(gdf_buildings['height'], errors='coerce').fillna(10)

crashes_df = pd.read_csv('/content/gdrive/MyDrive/CS 524/CS 524 - shared folder/Project/Dataset/Traffic_Crashes.csv', engine="pyarrow")



"""## 3. Analyze

### 3.1 Pie chart
"""

crashes_df['CRASH_DATE'] = pd.to_datetime(crashes_df['CRASH_DATE']).dt

crashes_df.head(2)

client = Socrata("data.cityofchicago.org", None)
results = client.get("unjd-c2ca")
results_df = pd.DataFrame.from_records(results)

results_df['the_geom'] = results_df['the_geom'].apply(lambda x: shape(x))

results_df["objectid"] = np.float64(results_df["objectid"])

zip_code_counts = crashes_df['Boundaries - ZIP Codes'].value_counts()

results_df['count'] = results_df['objectid'].map(zip_code_counts)

crashes_lighting = crashes_df.groupby(['LIGHTING_CONDITION', 'CRASH_MONTH']).size().reset_index(name='counts')
crashes_lighting = crashes_lighting[crashes_lighting['LIGHTING_CONDITION'] != 'UNKNOWN']

# total crashes of daylight over months
crashes_daylight = crashes_lighting[crashes_lighting['LIGHTING_CONDITION'] == 'DAYLIGHT'].drop(columns=['LIGHTING_CONDITION']).reset_index()

# total crashes of non-daylight over months
crashes_non_daylight = crashes_lighting[crashes_lighting['LIGHTING_CONDITION'] != 'DAYLIGHT'].groupby(['CRASH_MONTH']).sum('counts').reset_index()

# build data for pie chart over 12 months
crashes_light_data = {}
for month in crashes_daylight['CRASH_MONTH'].tolist():
  # Filter data for the current month
  daylight_count = crashes_daylight[crashes_daylight['CRASH_MONTH'] == month]['counts'].sum()
  non_daylight_count = crashes_non_daylight[crashes_non_daylight['CRASH_MONTH'] == month]['counts'].sum()

  # Add new data
  crashes_light_data[month] = [daylight_count, non_daylight_count]

# got the data and normalize the value
data1 = []
for month in list(crashes_light_data.keys()):
    values = crashes_light_data[month]
    total = sum(values)
    if total > 0:  # Prevent division by zero
        percentages = [int(x / total * 100) for x in values]
    else:
        percentages = [0, 0]  # Default to zero if no crashes

    data1.append(pd.DataFrame({"category": ["daylight", "non-daylight"], "value": percentages}))

from IPython.display import display


# List to store the individual charts
charts = []

# Create pie chart for each dataset
for idx, data in enumerate(data1):
    base = alt.Chart(data).encode(
        theta=alt.Theta("value:Q", stack=True),
        color=alt.Color("category:N")
    ).properties(width=210, height=210)

    pie = base.mark_arc(outerRadius=90)
    text = base.mark_text(radius=100, size=10).encode(text="value:N")


    chart = pie + text
    chart = chart.properties(title=f'Month {idx + 1}')
    charts.append(chart)

    if (idx + 1) % 4 == 0 or idx == len(data1) - 1:  # Check if 3 charts are added or it's the last chart
        display(alt.hconcat(*charts))
        charts = []  # Reset the charts list for the next row

# Display remaining charts (if not a multiple of 3)
if charts:
    display(alt.hconcat(*charts))

# Add a big title for everything at the end
big_title = alt.Chart().mark_text(text='Proportion of # accidents between daylight and non-daylight over months', size=15).properties(width=1000)
display(big_title)



"""### 3.2 Traffic Crashes Heatmap

"""

# Check if the necessary columns are present
if 'LATITUDE' not in crashes_df.columns or 'LONGITUDE' not in crashes_df.columns:
    raise ValueError("The dataset does not contain 'LATITUDE' and 'LONGITUDE' columns.")

# Ensure columns are numeric and not empty
crashes_df['LATITUDE'] = pd.to_numeric(crashes_df['LATITUDE'], errors='coerce')
crashes_df['LONGITUDE'] = pd.to_numeric(crashes_df['LONGITUDE'], errors='coerce')

# Drop rows with missing location data (if any)
crashes_df = crashes_df.dropna(subset=['LATITUDE', 'LONGITUDE'])

# Create a simplified dataframe
simplified_df = crashes_df[['LATITUDE', 'LONGITUDE']].copy()
simplified_df['weight'] = 1

# Create a heatmap layer without weight first
heatmap_layer = pdk.Layer(
    'HeatmapLayer',
    data=simplified_df,
    get_position=['LONGITUDE', 'LATITUDE'],
    get_weight='weight',  # Use a default weight column
    radiusPixels=30,
    color_range=[[255, 255, 178, 140], [254, 217, 118, 140], [253, 141, 60, 140], [240, 59, 32, 140], [189, 0, 38, 140]],
)

# Define the view state to center on downtown Chicago
view_state = pdk.ViewState(
    latitude=41.88,  # Center around downtown Chicago
    longitude=-87.63,
    zoom=14,  # Increased zoom for better downtown focus
    pitch=0,
    bearing=0
)

# Create the deck with the heatmap layer
deck = pdk.Deck(
    layers=[heatmap_layer],
    initial_view_state=view_state,
    map_style='dark',
)

# Display the heatmap
deck.show()

