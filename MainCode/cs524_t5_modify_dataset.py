# -*- coding: utf-8 -*-
"""CS524-T5-Modify_dataset

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aZFk-NbgzmwchTQi050fYhR1sP27QgSG

Dataset link:


*   [Traffic Crashes](https://data.cityofchicago.org/Transportation/Traffic-Crashes-Crashes/85ca-t3if/about_data)
*   [Chicago Street](https://data.cityofchicago.org/Transportation/Street-Center-Lines/6imu-meau)
*   [Chicago Boundary](https://data.cityofchicago.org/Facilities-Geographic-Boundaries/Boundaries-City/ewy2-6yfk)

## 0. Run the cells below before running any session
"""

import pandas as pd

from google.colab import drive
drive.mount('/content/gdrive')

path = "/content/gdrive/MyDrive/CS 524/CS 524 - shared folder/Project/Dataset"
output_path = "/content/gdrive/MyDrive/CS 524/CS 524 - shared folder/Project/Dataset/Clean_dataset"

"""## Clean the data in ChicagoBuildingHeight_OpenStreetMap and modified_ChicagoBuildingsHeight (only keep data we need)"""

ChicagoBuildingHeight_OpenStreetMap_Path = path + '/ChicagoBuildingsHeight_OpenStreetMap.csv'
modified_ChicagoBuildingsHeight_Path = output_path + '/clean_modified_ChicagoBuildingsHeight.csv'

# Load the datasets
openstreetmap_df = pd.read_csv(ChicagoBuildingHeight_OpenStreetMap_Path)
modified_buildings_df = pd.read_csv(modified_ChicagoBuildingsHeight_Path)

# Keep only rows in openstreetmap_df where 'id' matches with 'id' in modified_buildings_df
cleaned_openstreetmap_df = openstreetmap_df[openstreetmap_df['id'].isin(modified_buildings_df['id'])]

# Save the cleaned data to a new file if needed
data_output_path = output_path + '/Cleaned_ChicagoBuildingHeight_OpenStreetMap.csv'
cleaned_openstreetmap_df.to_csv(data_output_path, index=False)

print("Cleaned dataset saved to:", data_output_path)

"""## Merge Chicago Building Height with Chicago Street dataset based on street name, street direction and street type. Also add the street_id into Chicagi Building Height dataset."""

# Load the datasets
Building_Height_Path = output_path + '/clean_modified_ChicagoBuildingsHeight.csv'
Transportation_Path = path + '/transportation.csv'

# Load the datasets
buildings_df = pd.read_csv(Building_Height_Path)
streets_df = pd.read_csv(Transportation_Path)

# Normalize the case for matching columns
streets_df['PRE_DIR'] = streets_df['PRE_DIR'].str.upper()
streets_df['STREET_NAM'] = streets_df['STREET_NAM'].str.upper()
streets_df['STREET_TYP'] = streets_df['STREET_TYP'].str.upper()

buildings_df['addr:street:prefix'] = buildings_df['addr:street:prefix'].str.upper()
buildings_df['addr:street:name'] = buildings_df['addr:street:name'].str.upper()
buildings_df['addr:street:type'] = buildings_df['addr:street:type'].str.upper()

# Remove duplicates in Chicago_Streets based on the columns for matching
streets_df_dedup = streets_df.drop_duplicates(subset=['PRE_DIR', 'STREET_NAM', 'STREET_TYP'])

# Perform the merge based on the three matching conditions with deduplicated streets_df
merged_df = pd.merge(
    buildings_df,
    streets_df_dedup,
    left_on=['addr:street:prefix', 'addr:street:name', 'addr:street:type'],
    right_on=['PRE_DIR', 'STREET_NAM', 'STREET_TYP'],
    how='left'
)

# Add 'STREETNAME' from streets_df as 'street_id' in buildings_df where there is a match
# If no match, 'street_id' will automatically be NaN due to the 'left' join
buildings_df['street_id'] = merged_df['STREETNAME'].fillna('null')

# Save the result to a new file if needed
data_output_path = output_path + '/ChicagoBuildingHeight_with_street_id.csv'
buildings_df.to_csv(data_output_path, index=False)

print("Filtered dataset saved to:", data_output_path)

# Drop rows where 'street_id' is 'null'
buildings_df = buildings_df[buildings_df['street_id'] != 'null']

# Save the result to a new file if needed
data_output_path = output_path + '/Clean_ChicagoBuildingHeight_with_street_id.csv'
buildings_df.to_csv(data_output_path, index=False)

print("Filtered dataset saved to:", data_output_path)

"""Add some missing columns into Clean_ChicagoBuildingHeight_with_street_id.csv"""

# Load the datasets
Data_add_Path = output_path + '/Clean_ChicagoBuildingHeight_with_street_id.csv'
Data_original_Path = output_path + '/Cleaned_ChicagoBuildingHeight_OpenStreetMap.csv'

data_add_df = pd.read_csv(Data_add_Path)
data_original_df = pd.read_csv(Data_original_Path)

# Select and rename relevant columns in the original dataset for merging
data_original_df = data_original_df[['id', 'X', 'Y', 'height', 'name']].rename(columns={
    'X': 'building_X',
    'Y': 'building_Y',
    'name': 'building_name'
})

# Merge the datasets on the 'id' column
merged_df = data_add_df.merge(data_original_df, on='id', how='left')

# Save the merged dataset if needed
merged_df.to_csv(output_path + '/Clean_ChicagoBuildingHeight_with_street_id.csv', index=False)

# Display the first few rows to verify the merge
print(merged_df.head())

"""Rearrange the order of columns in Clean_ChicagoBuildingHeight_with_street_id.csv"""

# List all column names
column_names = merged_df.columns.tolist()
print("Column names:", column_names)

# Rename the 'height' column to 'building_height'
merged_df = merged_df.rename(columns={'height_x': 'building_height'})

# Define the desired column order
desired_column_order = [
    'id', 'building_X', 'building_Y', 'building_name',
    'addr:street', 'addr:street:prefix', 'addr:street:name', 'addr:street:type', 'street_id',
    'building_height', 'geometry', 'Floor'
]

# Reorder the columns in the DataFrame
merged_df = merged_df[desired_column_order]

# Display the first few rows to verify the new column order
print(merged_df.head())

# Save the merged dataset if needed
merged_df.to_csv(output_path + '/Clean_ChicagoBuildingHeight_with_street_id.csv', index=False)

"""## Create a unique street_segment_id in the transportation dataset, which accounts for each segment of a street

By grouping by STREETNAME and assigning a sequential number within each street, we generate a meaningful identifier that's both concise and unique within each street.
"""

Chicago_Streets_Path = path + '/Chicago_Streets.csv'
streets_df = pd.read_csv(Chicago_Streets_Path)

# Convert 'STREETNAME' to a string to ensure compatibility when concatenating
streets_df['STREETNAME'] = streets_df['STREETNAME'].astype(str)

# Generate a unique segment number within each STREETNAME group
streets_df['segment_number'] = streets_df.groupby('STREETNAME').cumcount() + 1

# Combine 'STREETNAME' and 'segment_number' to create the unique 'street_segment_id'
streets_df['street_segment_id'] = streets_df['STREETNAME'] + '_' + streets_df['segment_number'].astype(str)


# Save the updated dataset with the new 'street_segment_id' column
data_output_path = output_path + '/Chicago_Streets_with_street_segment_id.csv'
streets_df.to_csv(data_output_path, index=False)

print("Updated transportation dataset with street_segment_id saved to:", data_output_path)

"""Check if there is any duplicate 'street_segment_id'"""

Chicago_Streets_with_street_segment_id_Path = output_path + '/Chicago_Streets_with_street_segment_id.csv'
streets_with_street_segment_id_df = pd.read_csv(Chicago_Streets_with_street_segment_id_Path)

# Check for duplicates in 'street_segment_id'
duplicate_ids = streets_with_street_segment_id_df.duplicated(subset=['street_segment_id'], keep=False)

# Filter and display duplicates if any are found
duplicates = streets_with_street_segment_id_df[duplicate_ids]

# Display results
if not duplicates.empty:
    print("Duplicate 'street_segment_id' found:")
    print(duplicates[['street_segment_id']].value_counts())
else:
    print("No duplicate 'street_segment_id' found.")

# Check the number of rows in the dataset
num_rows = streets_with_street_segment_id_df.shape[0]
print("Number of rows in the streets_with_street_segment_id dataset:", num_rows)

num_rows = streets_df.shape[0]
print("Number of rows in the streets dataset:", num_rows)

"""## Decide which building belongs to which street segment"""

import geopandas as gpd
from shapely import wkt, geometry

# Load the datasets
streets_with_id_path = output_path + '/Chicago_Streets_with_street_segment_id.csv'
buildings_with_street_id_path = output_path + '/Clean_ChicagoBuildingHeight_with_street_id.csv'

streets_with_id_df = pd.read_csv(streets_with_id_path)
buildings_with_street_id_df = pd.read_csv(buildings_with_street_id_path)

# Convert geometry columns to GeoPandas geometries
streets_with_id_df['geometry'] = streets_with_id_df['the_geom'].apply(wkt.loads)
buildings_with_street_id_df['geometry'] = buildings_with_street_id_df['geometry'].apply(wkt.loads)

# Create GeoDataFrames
streets_gdf = gpd.GeoDataFrame(streets_with_id_df, geometry='geometry')
buildings_gdf = gpd.GeoDataFrame(buildings_with_street_id_df, geometry='geometry')

# Set CRS (coordinate reference system) for both GeoDataFrames
streets_gdf.set_crs('EPSG:4326', inplace=True)
buildings_gdf.set_crs('EPSG:4326', inplace=True)

# Rename street columns for clarity
streets_gdf = streets_gdf.rename(columns={
    'SHAPE_LEN': 'street_shape_len',
    'EWNS_DIR': 'street_ewns_dir',
    'LENGTH': 'street_length',
    'FNODE_ID': 'street_fnode_id',
    'TNODE_ID': 'street_tnode_id',
    'the_geom': 'street_geom',
    'OBJECTID': 'street_object_id',
    'TRANS_ID': 'street_trans_id'
})

# Columns to keep from streets data
streets_columns = [
    'street_segment_id', 'street_shape_len', 'street_ewns_dir', 'street_length',
    'street_fnode_id', 'street_tnode_id', 'street_geom', 'street_object_id', 'street_trans_id'
]

# Ensure that 'geometry' is retained in the subset for the spatial join
streets_subset_gdf = streets_gdf[streets_columns + ['geometry']]

# Convert 'building_X' and 'building_Y' to point geometries in the buildings GeoDataFrame
buildings_gdf['building_point'] = buildings_gdf.apply(lambda row: geometry.Point(row['building_X'], row['building_Y']), axis=1)

# Perform the nearest join instead of intersect
joined_gdf = gpd.sjoin_nearest(buildings_gdf, streets_subset_gdf, how='left', distance_col='distance')

# joined_gdf = gpd.sjoin(buildings_gdf, streets_subset_gdf, how='left', predicate='intersects')

# # Retain all building columns and the selected street columns
# result_df = joined_gdf[buildings_with_street_id_df.columns.tolist() + streets_columns]

# Filter rows to keep only those where the building point lies within the assigned street segment geometry
result_df = joined_gdf[joined_gdf.apply(lambda row: row['geometry'].contains(row['building_point']), axis=1)]

# Retain all building columns and the selected street columns
result_df = result_df[buildings_with_street_id_df.columns.tolist() + streets_columns]

# Check if any street columns, like 'street_ewns_dir', are still NaN
missing_ewns_dir = result_df['street_ewns_dir'].isna().sum()
print(f"Rows with missing 'street_ewns_dir': {missing_ewns_dir}")

# Print the number of rows in the DataFrame
num_rows = joined_gdf.shape[0]
print("Number of rows in the dataset:", num_rows)

# Display the first few rows to check the matches
print(result_df.head())

# Save to a new CSV file if desired
result_df.to_csv(output_path + '/matched_buildings_with_street_segments.csv', index=False)

"""## Merge building data (with street_segment_id) with shadow data"""

building_data_path = output_path + '/matched_buildings_with_street_segments.csv'
shadow_data_path = output_path + '/5PM_Dataset/shadows_street_info_5pm.csv'

building_df = pd.read_csv(building_data_path)
shadow_df = pd.read_csv(shadow_data_path)

# Select and rename relevant columns in the shadow dataset for merging
shadow_df = shadow_df[['id', 'building_id', 'geometry', 'type']].rename(columns={
    'building_id': 'D_label_building_id',
    'geometry': 'shadow_geometry',
    'type': 'shadow_type'
})

# Merge the datasets on the 'id' column
merged_building_shadow_df = building_df.merge(shadow_df, on='id', how='left')

# Display the first few rows to verify the merge
print(merged_building_shadow_df.head())

# Optionally, save the merged result
merged_building_shadow_df.to_csv(output_path +'/Complete_buildings_in_street_segment_id_with_shadow_data.csv', index=False)

# Print the number of rows in the DataFrame
num_rows = building_df.shape[0]
print("Number of rows in the building dataset:", num_rows)

# Print the number of rows in the DataFrame
num_rows = shadow_df.shape[0]
print("Number of rows in the shadow dataset:", num_rows)

# Print the number of rows in the DataFrame
num_rows = merged_building_shadow_df.shape[0]
print("Number of rows in the result dataset:", num_rows)