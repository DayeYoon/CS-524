# -*- coding: utf-8 -*-
"""CS524-T4-Sunshine-for-every-street-segments.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GLx4qXXYzgPBaRVXhWsdPNQpCIqnMqeD

Dataset link:


*   [Traffic Crashes](https://data.cityofchicago.org/Transportation/Traffic-Crashes-Crashes/85ca-t3if/about_data)
*   [Chicago Street](https://data.cityofchicago.org/Transportation/Street-Center-Lines/6imu-meau)
*   [Chicago Boundary](https://data.cityofchicago.org/Facilities-Geographic-Boundaries/Boundaries-City/ewy2-6yfk)

## 0. Run the cells below before running any session
"""

!pip install pandas pysolar pytz
!pip install sodapy
!pip install geojson
!pip install pydeck

import pandas as pd
import requests
import altair as alt
import matplotlib.pyplot as plt
from sodapy import Socrata
from shapely.geometry import Polygon, MultiPolygon
from shapely.geometry import shape
import geojson
import numpy as np

from google.colab import drive
drive.mount('/content/gdrive')

path = "/content/gdrive/MyDrive/CS 524/CS 524 - shared folder/Project/Dataset"

import pydeck as pdk
import geopandas as gpd
from shapely.geometry import Point, LineString, Polygon

# Load the datasets
Building_Height_File = path + '/ChicagoBuildingsHeight_OpenStreetMap.geojson'
Chicago_Boundary_File = path + '/Chicago_City_Boundary.csv'
Chicago_Streets_File = path + '/Chicago_Streets.csv'

gdf_buildings = gpd.read_file(Building_Height_File)

# Load and process the boundary CSV
df_boundary = pd.read_csv(Chicago_Boundary_File)

# Load and process the streets CSV
df_streets = pd.read_csv(Chicago_Streets_File)

# Convert the 'the_geom' column to a geometry object
gdf_boundary = gpd.GeoDataFrame(df_boundary, geometry=gpd.GeoSeries.from_wkt(df_boundary['the_geom']))
# Convert the 'the_geom' column to a geometry object
gdf_streets = gpd.GeoDataFrame(df_streets, geometry=gpd.GeoSeries.from_wkt(df_streets['the_geom']))

# Set an initial CRS for boundary and streets if not already set
gdf_boundary = gdf_boundary.set_crs('EPSG:4326')
gdf_streets = gdf_streets.set_crs('EPSG:4326')

gdf_buildings = gdf_buildings.to_crs('EPSG:4326')
gdf_boundary = gdf_boundary.to_crs('EPSG:4326')
gdf_streets = gdf_streets.to_crs('EPSG:4326')

# Convert height to numeric, replacing non-numeric values with a default height
gdf_buildings['height'] = pd.to_numeric(gdf_buildings['height'], errors='coerce').fillna(10)

crashes_df = pd.read_csv('/content/gdrive/MyDrive/CS 524/CS 524 - shared folder/Project/Dataset/Traffic_Crashes.csv', engine="pyarrow")

"""## 4. Calculate the sunshine for every street segments"""

# import pandas as pd
import traceback
from shapely import wkt
from math import atan2, degrees
import json
import os

# Define the dataset path
dataset_path = path

# --------------------------------------------------------------------------------------------------------------------------------
# Divide 3 part
# Boi: 0 - 3333
# jsonl_filename = os.path.join(dataset_path + '/Chicago_Sunshine_Angle.jsonl')
# progress_filename = os.path.join(dataset_path + '/progress.json')

# Daye: 3334 - 6667
# jsonl_filename = os.path.join(dataset_path + '/Chicago_Sunshine_Angle_1.jsonl')
# progress_filename = os.path.join(dataset_path + '/progress_1.json')

# Hoang: 6668 - 10001
# part 1: 6668 - 9199
jsonl_filename = os.path.join(dataset_path + '/Chicago_Sunshine_Angle_2.jsonl')
progress_filename = os.path.join(dataset_path + '/progress_2.json')
# part 2: 9200 - rest
# jsonl_filename = os.path.join(dataset_path + '/Chicago_Sunshine_Angle_2_part2.jsonl')
# progress_filename = os.path.join(dataset_path + '/progress_2_part2.json')
# --------------------------------------------------------------------------------------------------------------------------------

print("Check here",progress_filename)

# Function to calculate the bearing (orientation) of a street segment (handles both LINESTRING and MULTILINESTRING)
def calculate_bearing_multiline(geometry):
    if geometry.is_empty:
        return None

    if geometry.geom_type == 'LineString':
        start_point = geometry.coords[0]
        end_point = geometry.coords[-1]
    elif geometry.geom_type == 'MultiLineString':
        first_line = geometry.geoms[0]
        last_line = geometry.geoms[-1]
        start_point = first_line.coords[0]
        end_point = last_line.coords[-1]
    else:
        return None

    # Calculate bearing using atan2
    delta_x = end_point[0] - start_point[0]
    delta_y = end_point[1] - start_point[1]

    angle = atan2(delta_y, delta_x)
    bearing = degrees(angle)

    # Normalize bearing to 0-360 degrees
    if bearing < 0:
        bearing += 360
    return bearing

# Function to calculate the angle between the sun and the street segment
def calculate_sunshine_angle(sun_azimuth, street_bearing):
    angle_diff = abs(sun_azimuth - street_bearing)
    if angle_diff > 180:
        angle_diff = 360 - angle_diff
    return angle_diff

# Function to load progress
def load_progress():
    if os.path.exists(progress_filename):
        with open(progress_filename, 'r') as f:
            progress = json.load(f)
        return progress.get('last_index', 0)
    return 0

# Function to save progress
def save_progress(last_index):
    with open(progress_filename, 'w') as f:
        json.dump({'last_index': last_index}, f)

# Function to append a single result to the JSONL file
def append_to_jsonl(data):
    with open(jsonl_filename, 'a') as f:
        f.write(json.dumps(data) + '\n')

# Load street data and sun data
street_data = pd.read_csv(os.path.join(dataset_path + '/Chicago_Streets.csv'))
building_data = pd.read_csv(os.path.join(dataset_path + '/shadow_positions.csv'))

# Parse street geometries and calculate bearings
street_data['geometry'] = street_data['the_geom'].apply(wkt.loads)
street_data['bearing'] = street_data['geometry'].apply(calculate_bearing_multiline)

# Ask the user to input the desired month (in MM format)
# month_to_calculate = input("Enter the month (MM) to calculate: ").zfill(2)

# Convert datetime in the building data to datetime format
building_data['datetime'] = pd.to_datetime(building_data['datetime'])

# Print out the first few rows of the building data to check its structure
print("Building Data Sample:")
print(building_data.head())

# Filter the building data to only include records from the desired month
# building_data_filtered = building_data[building_data['datetime'].dt.strftime('%m') == month_to_calculate]

# Check how many records we have after filtering
# print(f"Number of records for month {month_to_calculate}: {len(building_data_filtered)}")

# Load the last processed index
start_index = load_progress()
print(f"Resuming from index: {start_index}")

# stop at this line

# Initialize results list (optional, if you want to keep some in-memory)
# results = []
write_to_json = open(jsonl_filename, 'a')
save_progress_t = open(progress_filename, 'w')


# Loop through each timestamp in the filtered sun data starting from the last index
try:
    for idx, row in building_data.iloc[start_index:].iterrows():
        current_index = idx  # Assuming idx is a unique identifier or sequential index

        sun_azimuth = row['sun_azimuth_deg']
        sun_altitude = row['sun_altitude_deg']
        timestamp = row['datetime']

        # Skip if sun altitude is below the horizon (altitude < 0)
        if sun_altitude < 0:
            continue

        for i, street in street_data.iterrows():
            street_bearing = street['bearing']
            street_name = street['STREETNAME']

            # Calculate the sunshine angle for the current street segment
            sunshine_angle = calculate_sunshine_angle(sun_azimuth, street_bearing)

            # Create the result dictionary
            result = {
                'timestamp': timestamp.isoformat(),
                'street_name': street_name,
                'sun_azimuth': sun_azimuth,
                'sun_altitude': sun_altitude,
                'street_bearing': street_bearing,
                'sunshine_angle': sunshine_angle
            }

            # Append the result to the JSONL file
            # append_to_jsonl(result)

            write_to_json.write(json.dumps(result) + '\n')

        # Update progress after processing each row
        save_progress(idx + 1)
        # json.dump({'last_index': idx + 1}, progress_filename)

        if (idx + 1) % 100 == 0:
            print(f"Processed {idx + 1} records.")

except Exception as e:
    traceback.print_exc()
    print(f"An error occurred: {e}")
    print("Progress has been saved. You can resume the script later.")

# Final message after processing
print(f"Processing complete for month. Results saved to {jsonl_filename}")